{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88f449d",
   "metadata": {},
   "source": [
    "# **Quinto conjunto de tareas a realizar**\n",
    "\n",
    "## Paquetes necesarios e inicializaciones\n",
    "\n",
    "La siguiente práctica consta de dos partes principales, la primera es entrenar una red neuronal para diferenciar características faciales y tras esto, crear un filtro que use las soluciones de la red, y la segunda, consiste en la realización de un filtro de ámbito libre.\n",
    "\n",
    "Para la realización de las tareas será necesario realizar las siguientes instalaciones y creación de un nuevo enviroment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3aa53a6d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "conda create --name VC_P5 python=3.11.5\n",
    "conda activate VC_P5\n",
    "pip install mediapipe matplotlib imutils mtcnn scikit-learn scikit-image pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a90d0",
   "metadata": {},
   "source": [
    "Se van a realizar las importaciones necesarias para la ejecución de los consiguientes fragmentos de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f2c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image # Usaremos Pillow en lugar de tf.keras.utils\n",
    "import cv2 # Usaremos OpenCV para guardar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "# from retinaface import RetinaFace\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Para el scheduler\n",
    "from tqdm import tqdm # Para las barras de progreso\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e197534",
   "metadata": {},
   "source": [
    "En el siguiente fragmento se procede a crear una carpeta con las imágenes divididas en \"test\", \"train\" y \"validation\" para el posterior entrenemiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76331a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta 'organized_data' no encontrada. Iniciando organización...\n",
      "Iniciando carga de datos desde la carpeta original...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCarpeta \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mORGANIZED_DATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m no encontrada. Iniciando organización...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     X, y = \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mORIGINAL_DATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDividiendo datos...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m     X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=\u001b[32m0.3\u001b[39m, stratify=y, random_state=\u001b[32m42\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mload_and_preprocess_data\u001b[39m\u001b[34m(path, img_size)\u001b[39m\n\u001b[32m     29\u001b[39m full_path = os.path.join(path, filename)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Usamos Pillow (PIL) para cargar y redimensionar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m img = img.resize((img_size, img_size), Image.Resampling.LANCZOS)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Convertimos a array numpy y normalizamos\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\PIL\\Image.py:984\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mBGR;15\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;16\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBGR;24\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    982\u001b[39m     deprecate(mode, \u001b[32m12\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\PIL\\ImageFile.py:280\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m         s = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct.error) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    282\u001b[39m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[32m    283\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P5\\Lib\\site-packages\\PIL\\JpegImagePlugin.py:414\u001b[39m, in \u001b[36mJpegImageFile.load_read\u001b[39m\u001b[34m(self, read_bytes)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, read_bytes: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m    409\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    410\u001b[39m \u001b[33;03m    internal: read more image data\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    For premature EOF and LOAD_TRUNCATED_IMAGES adds EOI marker\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m    so libjpeg can finish decoding\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     s = \u001b[38;5;28mself\u001b[39m.fp.read(read_bytes)\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m ImageFile.LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_ended\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    417\u001b[39m         \u001b[38;5;66;03m# Premature EOF.\u001b[39;00m\n\u001b[32m    418\u001b[39m         \u001b[38;5;66;03m# Pretend file is finished adding EOI marker\u001b[39;00m\n\u001b[32m    419\u001b[39m         \u001b[38;5;28mself\u001b[39m._ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 1. Configuración de Preparación ---\n",
    "IMG_SIZE = 224\n",
    "# Ruta a la carpeta original en tu escritorio\n",
    "ORIGINAL_DATA_PATH = \"C:/Users/ivanp/Desktop/UTKFaces/\" \n",
    "# Carpeta donde se guardarán los datos divididos\n",
    "ORGANIZED_DATA_DIR = \"organized_data\" \n",
    "CLASS_NAMES = [\"joven\", \"medio\", \"anciano\"]\n",
    "\n",
    "# --- 2. Funciones de Carga y Guardado (versión PyTorch/Pillow) ---\n",
    "\n",
    "def get_label_from_age(age):\n",
    "    if 0 <= age <= 29: return 0  # Joven\n",
    "    elif 30 <= age <= 59: return 1  # Medio\n",
    "    elif age >= 60: return 2  # Anciano\n",
    "    return None\n",
    "\n",
    "def load_and_preprocess_data(path, img_size):\n",
    "    images = []\n",
    "    labels = []\n",
    "    print(\"Iniciando carga de datos desde la carpeta original...\")\n",
    "    for filename in os.listdir(path):\n",
    "        if not filename.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            continue\n",
    "        try:\n",
    "            age_str = filename.split('_')[0]\n",
    "            age = int(age_str)\n",
    "            label = get_label_from_age(age)\n",
    "            if label is not None:\n",
    "                full_path = os.path.join(path, filename)\n",
    "                \n",
    "                # Usamos Pillow (PIL) para cargar y redimensionar\n",
    "                img = Image.open(full_path).convert('RGB')\n",
    "                img = img.resize((img_size, img_size), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                # Convertimos a array numpy y normalizamos\n",
    "                img_array = np.array(img) / 255.0\n",
    "                \n",
    "                images.append(img_array) \n",
    "                labels.append(label)\n",
    "        except Exception as e:\n",
    "            # print(f\"Error cargando {filename}: {e}\")\n",
    "            pass\n",
    "            \n",
    "    print(f\"Carga finalizada. Total de imágenes: {len(images)}\")\n",
    "    # Convertimos a float32 para PyTorch\n",
    "    X = np.array(images, dtype=np.float32) \n",
    "    y = np.array(labels)\n",
    "    return X, y\n",
    "\n",
    "def save_split_data_to_folders(X_data, y_data, split_name, base_dir):\n",
    "    print(f\"\\nGuardando imágenes del conjunto: {split_name}...\")\n",
    "    for i, (img, label) in enumerate(zip(X_data, y_data)):\n",
    "        class_name = CLASS_NAMES[label]\n",
    "        target_dir = os.path.join(base_dir, split_name, class_name)\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "            \n",
    "        filename = f\"img_{i}.png\"\n",
    "        filepath = os.path.join(target_dir, filename)\n",
    "        \n",
    "        # Usamos OpenCV (cv2) para guardar (revirtiendo normalización)\n",
    "        # cv2 espera BGR, así que convertimos RGB -> BGR\n",
    "        img_bgr = cv2.cvtColor((img * 255.0).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "        cv2.imwrite(filepath, img_bgr)\n",
    "        \n",
    "    print(f\"Imágenes de {split_name} guardadas en {base_dir}/{split_name}\")\n",
    "\n",
    "# --- 3. Ejecución Principal (Solo si se ejecuta este script) ---\n",
    "# (Este 'if' es para que puedas importar las funciones en otros scripts si lo necesitas)\n",
    "if __name__ == \"__main__\":\n",
    "    if os.path.exists(ORGANIZED_DATA_DIR):\n",
    "        print(f\"La carpeta '{ORGANIZED_DATA_DIR}' ya existe. No se necesita preparación.\")\n",
    "    else:\n",
    "        print(f\"Carpeta '{ORGANIZED_DATA_DIR}' no encontrada. Iniciando organización...\")\n",
    "        \n",
    "        X, y = load_and_preprocess_data(ORIGINAL_DATA_PATH, IMG_SIZE)\n",
    "        \n",
    "        print(\"Dividiendo datos...\")\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "        print(\"División completa.\")\n",
    "        \n",
    "        save_split_data_to_folders(X_train, y_train, \"train\", base_dir=ORGANIZED_DATA_DIR)\n",
    "        save_split_data_to_folders(X_val, y_val, \"validation\", base_dir=ORGANIZED_DATA_DIR)\n",
    "        save_split_data_to_folders(X_test, y_test, \"test\", base_dir=ORGANIZED_DATA_DIR)\n",
    "        \n",
    "        print(\"\\n--- ¡Organización de carpetas completa! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f2c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verificando la disponibilidad de la GPU (PyTorch) ---\n",
      "✅ GPU detectada: NVIDIA GeForce RTX 3060 Ti\n",
      "Usando dispositivo: cuda\n",
      "--------------------------------------------------\n",
      "Cargando datos desde la carpeta 'C:/Users/ivanp/Desktop/organized_data/'...\n",
      "Clases encontradas: ['anciano', 'joven', 'medio']\n",
      "Imágenes de entrenamiento: 7095, Validación: 1521\n",
      "Carga de datos finalizada.\n",
      "\n",
      "Logs de cada época se guardarán en: training_log.csv\n",
      "El mejor modelo se guardará en: best_age_model.pth\n",
      "\n",
      "--- Iniciando Fase 1: Entrenamiento de la Capa Final (15 épocas) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Train Loss: 0.6384, Train Acc: 73.15% | Val Loss: 0.5414, Val Acc: 77.38%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 0.00% -> 77.38%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | Train Loss: 0.5392, Train Acc: 77.15% | Val Loss: 0.5950, Val Acc: 75.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | Train Loss: 0.5201, Train Acc: 78.60% | Val Loss: 0.4547, Val Acc: 80.41%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 77.38% -> 80.41%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | Train Loss: 0.5028, Train Acc: 78.76% | Val Loss: 0.4582, Val Acc: 80.54%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 80.41% -> 80.54%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | Train Loss: 0.4893, Train Acc: 79.01% | Val Loss: 0.4947, Val Acc: 79.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 | Train Loss: 0.4862, Train Acc: 78.87% | Val Loss: 0.4479, Val Acc: 80.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 | Train Loss: 0.4797, Train Acc: 79.49% | Val Loss: 0.4440, Val Acc: 81.20%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 80.54% -> 81.20%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 | Train Loss: 0.4750, Train Acc: 79.32% | Val Loss: 0.4393, Val Acc: 81.66%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 81.20% -> 81.66%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 | Train Loss: 0.4710, Train Acc: 79.45% | Val Loss: 0.4602, Val Acc: 80.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 | Train Loss: 0.4611, Train Acc: 80.07% | Val Loss: 0.4343, Val Acc: 81.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 | Train Loss: 0.4548, Train Acc: 80.59% | Val Loss: 0.4502, Val Acc: 80.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 | Train Loss: 0.4598, Train Acc: 80.13% | Val Loss: 0.4449, Val Acc: 81.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 | Train Loss: 0.4293, Train Acc: 81.01% | Val Loss: 0.4295, Val Acc: 81.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 | Train Loss: 0.4178, Train Acc: 82.31% | Val Loss: 0.4266, Val Acc: 81.92%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 81.66% -> 81.92%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 | Train Loss: 0.4130, Train Acc: 82.71% | Val Loss: 0.4272, Val Acc: 82.05%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 81.92% -> 82.05%)\n",
      "\n",
      "--- Iniciando Fase 2: Ajuste Fino de todo el modelo (50 épocas) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/65 | Train Loss: 0.3941, Train Acc: 83.47% | Val Loss: 0.3803, Val Acc: 83.89%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 82.05% -> 83.89%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/65 | Train Loss: 0.2941, Train Acc: 87.58% | Val Loss: 0.3679, Val Acc: 84.75%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 83.89% -> 84.75%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/65 | Train Loss: 0.2371, Train Acc: 90.37% | Val Loss: 0.3671, Val Acc: 84.88%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 84.75% -> 84.88%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/65 | Train Loss: 0.1752, Train Acc: 93.26% | Val Loss: 0.3869, Val Acc: 85.40%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 84.88% -> 85.40%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/65 | Train Loss: 0.1267, Train Acc: 95.93% | Val Loss: 0.4041, Val Acc: 85.34%\n",
      "  Sin mejora. 1/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/65 | Train Loss: 0.1019, Train Acc: 96.63% | Val Loss: 0.4106, Val Acc: 86.06%\n",
      "  ¡Mejora! Guardando modelo... (Val Acc: 85.40% -> 86.06%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/65 | Train Loss: 0.0699, Train Acc: 97.87% | Val Loss: 0.4525, Val Acc: 86.00%\n",
      "  Sin mejora. 1/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/65 | Train Loss: 0.0553, Train Acc: 98.38% | Val Loss: 0.4852, Val Acc: 85.54%\n",
      "  Sin mejora. 2/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/65 | Train Loss: 0.0430, Train Acc: 98.77% | Val Loss: 0.5096, Val Acc: 85.34%\n",
      "  Sin mejora. 3/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/65 | Train Loss: 0.0339, Train Acc: 99.01% | Val Loss: 0.5207, Val Acc: 85.14%\n",
      "  Sin mejora. 4/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/65 | Train Loss: 0.0285, Train Acc: 99.27% | Val Loss: 0.5870, Val Acc: 85.14%\n",
      "  Sin mejora. 5/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/65 | Train Loss: 0.0261, Train Acc: 99.38% | Val Loss: 0.6090, Val Acc: 85.08%\n",
      "  Sin mejora. 6/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/65 | Train Loss: 0.0207, Train Acc: 99.44% | Val Loss: 0.5695, Val Acc: 85.47%\n",
      "  Sin mejora. 7/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/65 | Train Loss: 0.0191, Train Acc: 99.48% | Val Loss: 0.5809, Val Acc: 85.40%\n",
      "  Sin mejora. 8/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/65 | Train Loss: 0.0162, Train Acc: 99.56% | Val Loss: 0.5804, Val Acc: 85.14%\n",
      "  Sin mejora. 9/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/65 | Train Loss: 0.0133, Train Acc: 99.76% | Val Loss: 0.5759, Val Acc: 85.67%\n",
      "  Sin mejora. 10/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/65 | Train Loss: 0.0134, Train Acc: 99.70% | Val Loss: 0.5964, Val Acc: 85.47%\n",
      "  Sin mejora. 11/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/65 | Train Loss: 0.0106, Train Acc: 99.77% | Val Loss: 0.5811, Val Acc: 85.27%\n",
      "  Sin mejora. 12/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/65 | Train Loss: 0.0121, Train Acc: 99.76% | Val Loss: 0.6043, Val Acc: 85.34%\n",
      "  Sin mejora. 13/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/65 | Train Loss: 0.0124, Train Acc: 99.68% | Val Loss: 0.5828, Val Acc: 85.34%\n",
      "  Sin mejora. 14/15 épocas para Early Stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/65 | Train Loss: 0.0125, Train Acc: 99.69% | Val Loss: 0.5929, Val Acc: 85.21%\n",
      "  Sin mejora. 15/15 épocas para Early Stop.\n",
      "\n",
      "--- ¡Early Stopping! ---\n",
      "No hubo mejora en las últimas 15 épocas.\n",
      "Parando entrenamiento.\n",
      "\n",
      "--- Entrenamiento Completado ---\n",
      "Resumen final del historial COMPLETO guardado en: training_history_summary.csv\n",
      "\n",
      "Cargando el mejor modelo guardado (best_age_model.pth) para la evaluación...\n",
      "Evaluando el modelo en el conjunto de prueba...\n",
      "Test Loss: 0.4258 Acc: 0.8402\n",
      "Precisión final en el conjunto de prueba: 84.02%\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Verificación de GPU (PyTorch) ---\n",
    "print(\"--- Verificando la disponibilidad de la GPU (PyTorch) ---\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU detectada: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"---------------------------------------------------------\")\n",
    "    print(\">>> ⚠️ ¡ADVERTENCIA! No se encontró ninguna GPU. <<<\")\n",
    "    print(\">>> El entrenamiento se ejecutará en la CPU (será lento).\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 1. Configuración Inicial ---\n",
    "IMG_SIZE = 224\n",
    "ORGANIZED_DATA_DIR = \"C:/Users/ivanp/Desktop/organized_data/\"   # Cambiar según sea necesario\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# --- Configuración del Entrenamiento en 2 Fases ---\n",
    "EPOCHS_HEAD = 15\n",
    "EPOCHS_TUNE = 50 # Épocas SOLO para el ajuste fino (total = HEAD + TUNE)\n",
    "LR_HEAD = 0.001\n",
    "LR_TUNE = 1e-5\n",
    "EARLY_STOP_PATIENCE = 15 # Paciencia para la parada temprana\n",
    "\n",
    "# Archivos de salida\n",
    "csv_log_file = './train_results/training_log.csv'\n",
    "best_model_file = './train_results/best_age_model.pth'\n",
    "summary_csv_file = './train_results/training_history_summary.csv'\n",
    "\n",
    "# --- 3. Cargar Datos (Estilo PyTorch) ---\n",
    "print(f\"Cargando datos desde la carpeta '{ORGANIZED_DATA_DIR}'...\")\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder(os.path.join(ORGANIZED_DATA_DIR, 'train'), data_transforms['train']),\n",
    "    'val': datasets.ImageFolder(os.path.join(ORGANIZED_DATA_DIR, 'validation'), data_transforms['val']),\n",
    "    'test': datasets.ImageFolder(os.path.join(ORGANIZED_DATA_DIR, 'test'), data_transforms['test'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=4),\n",
    "    'test': DataLoader(image_datasets['test'], batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "print(f\"Clases encontradas: {class_names}\")\n",
    "print(f\"Imágenes de entrenamiento: {dataset_sizes['train']}, Validación: {dataset_sizes['val']}\")\n",
    "print(\"Carga de datos finalizada.\")\n",
    "\n",
    "\n",
    "# --- 4. Definir el Modelo (ResNet50 en PyTorch) ---\n",
    "model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Congelar la base (FASE 1)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, len(class_names))\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- 5. Lógica de Entrenamiento (2 Fases separadas) ---\n",
    "\n",
    "# Preparar CSV logger\n",
    "try:\n",
    "    with open(csv_log_file, 'w') as f:\n",
    "        f.write('epoch,train_loss,train_acc,val_loss,val_acc\\n')\n",
    "    print(f\"\\nLogs de cada época se guardarán en: {csv_log_file}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error al escribir CSV: {e}\")\n",
    "\n",
    "# Dataframe para el historial completo\n",
    "full_history_df = pd.DataFrame()\n",
    "best_val_acc = 0.0\n",
    "print(f\"El mejor modelo se guardará en: {best_model_file}\")\n",
    "\n",
    "# --- Fase 1: Entrenamiento de la Capa Final (Head) ---\n",
    "\n",
    "optimizer = optim.AdamW(model.fc.parameters(), lr=LR_HEAD, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)\n",
    "\n",
    "print(f\"\\n--- Iniciando Fase 1: Entrenamiento de la Capa Final ({EPOCHS_HEAD} épocas) ---\")\n",
    "\n",
    "for epoch in range(EPOCHS_HEAD):\n",
    "    model.train()\n",
    "    train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "    train_loop = tqdm(dataloaders['train'], desc=f\"Epoch {epoch+1}/{EPOCHS_HEAD} [Head Train]\", leave=False)\n",
    "    \n",
    "    for images, labels in train_loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validación de la Fase 1\n",
    "    model.eval()\n",
    "    val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloaders['val']:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(dataloaders['train'])\n",
    "    avg_train_acc = (correct_train / total_train)\n",
    "    avg_val_loss = val_loss / len(dataloaders['val'])\n",
    "    avg_val_acc = (correct_val / total_val)\n",
    "    \n",
    "    # Guardar en CSV y DF\n",
    "    try:\n",
    "        with open(csv_log_file, 'a') as f:\n",
    "            f.write(f'{epoch},{avg_train_loss:.4f},{avg_train_acc:.4f},{avg_val_loss:.4f},{avg_val_acc:.4f}\\n')\n",
    "        epoch_data = {'epoch': epoch, 'train_loss': avg_train_loss, 'train_acc': avg_train_acc, 'val_loss': avg_val_loss, 'val_acc': avg_val_acc}\n",
    "        full_history_df = pd.concat([full_history_df, pd.DataFrame([epoch_data])], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error guardando logs: {e}\")\n",
    "    \n",
    "    # Imprimir\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS_HEAD} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc*100:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Guardar Mejor Modelo\n",
    "    if avg_val_acc > best_val_acc:\n",
    "        print(f\"  ¡Mejora! Guardando modelo... (Val Acc: {best_val_acc*100:.2f}% -> {avg_val_acc*100:.2f}%)\")\n",
    "        torch.save(model.state_dict(), best_model_file)\n",
    "        best_val_acc = avg_val_acc\n",
    "    \n",
    "    scheduler.step(avg_val_acc)\n",
    "\n",
    "# --- Fase 2: Ajuste Fino (Fine-Tuning) ---\n",
    "\n",
    "print(f\"\\n--- Iniciando Fase 2: Ajuste Fino de todo el modelo ({EPOCHS_TUNE} épocas) ---\")\n",
    "\n",
    "# Descongelar TODAS las capas\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Nuevo optimizador para TODOS los parámetros, con LR muy baja\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_TUNE, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
    "\n",
    "# Reiniciar variables de Early Stopping\n",
    "epochs_no_improve = 0\n",
    "min_delta = 0.0001 # 0.01%\n",
    "\n",
    "for epoch in range(EPOCHS_TUNE):\n",
    "    current_total_epoch = epoch + EPOCHS_HEAD # Época total\n",
    "    \n",
    "    model.train()\n",
    "    train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "    train_loop = tqdm(dataloaders['train'], desc=f\"Epoch {current_total_epoch+1}/{EPOCHS_HEAD + EPOCHS_TUNE} [Tune Train]\", leave=False)\n",
    "    \n",
    "    for images, labels in train_loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validación de la Fase 2\n",
    "    model.eval()\n",
    "    val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloaders['val']:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(dataloaders['train'])\n",
    "    avg_train_acc = (correct_train / total_train)\n",
    "    avg_val_loss = val_loss / len(dataloaders['val'])\n",
    "    avg_val_acc = (correct_val / total_val)\n",
    "    \n",
    "    # Guardar en CSV y DF\n",
    "    try:\n",
    "        with open(csv_log_file, 'a') as f:\n",
    "            f.write(f'{current_total_epoch},{avg_train_loss:.4f},{avg_train_acc:.4f},{avg_val_loss:.4f},{avg_val_acc:.4f}\\n')\n",
    "        epoch_data = {'epoch': current_total_epoch, 'train_loss': avg_train_loss, 'train_acc': avg_train_acc, 'val_loss': avg_val_loss, 'val_acc': avg_val_acc}\n",
    "        full_history_df = pd.concat([full_history_df, pd.DataFrame([epoch_data])], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error guardando logs: {e}\")\n",
    "    \n",
    "    # Imprimir\n",
    "    print(f\"Epoch {current_total_epoch+1}/{EPOCHS_HEAD + EPOCHS_TUNE} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc*100:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc*100:.2f}%\")\n",
    "\n",
    "    # Lógica de Scheduler y Early Stopping\n",
    "    scheduler.step(avg_val_acc)\n",
    "    \n",
    "    if avg_val_acc > (best_val_acc + min_delta):\n",
    "        print(f\"  ¡Mejora! Guardando modelo... (Val Acc: {best_val_acc*100:.2f}% -> {avg_val_acc*100:.2f}%)\")\n",
    "        torch.save(model.state_dict(), best_model_file)\n",
    "        best_val_acc = avg_val_acc\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  Sin mejora. {epochs_no_improve}/{EARLY_STOP_PATIENCE} épocas para Early Stop.\")\n",
    "\n",
    "    if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\n--- ¡Early Stopping! ---\")\n",
    "        print(f\"No hubo mejora en las últimas {EARLY_STOP_PATIENCE} épocas.\")\n",
    "        print(f\"Parando entrenamiento.\")\n",
    "        break # Salir del bucle de Fase 2\n",
    "\n",
    "print(\"\\n--- Entrenamiento Completado ---\")\n",
    "\n",
    "# --- 7. Guardar Historial y Evaluar ---\n",
    "\n",
    "# Guardar el historial completo en CSV\n",
    "try:\n",
    "    full_history_df.to_csv(summary_csv_file, index=False)\n",
    "    print(f\"Resumen final del historial COMPLETO guardado en: {summary_csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo guardar el resumen del historial: {e}\")\n",
    "\n",
    "# Cargar el mejor modelo para la evaluación final\n",
    "try:\n",
    "    print(f\"\\nCargando el mejor modelo guardado ({best_model_file}) para la evaluación...\")\n",
    "    model.load_state_dict(torch.load(best_model_file))\n",
    "except Exception as e:\n",
    "    print(f\"No se encontró el mejor modelo en '{best_model_file}'. Usando el último modelo en memoria. Error: {e}\")\n",
    "\n",
    "model.eval() \n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "print(\"Evaluando el modelo en el conjunto de prueba...\")\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloaders['test']:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "test_loss = running_loss / dataset_sizes['test']\n",
    "test_acc = running_corrects.double() / dataset_sizes['test']\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}')\n",
    "print(f\"Precisión final en el conjunto de prueba: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3535b",
   "metadata": {},
   "source": [
    "Obtener gráficos a partir de los resultados del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeb82b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando gráficos desde ./train_results/training_history_summary.csv...\n",
      "Gráfico de Precisión guardado en: .\\./train_results/training_accuracy_plot.png\n",
      "Gráfico de Pérdida guardado en: .\\./train_results/training_loss_plot.png\n",
      "\n",
      "Proceso de generación de gráficos finalizado.\n"
     ]
    }
   ],
   "source": [
    "def plot_training_history(csv_path, head_epochs, save_dir=\".\"):\n",
    "    \"\"\"\n",
    "    Lee el historial de entrenamiento desde un archivo CSV y genera gráficos\n",
    "    de precisión y pérdida, guardándolos como archivos PNG.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Ruta al archivo 'training_history_summary.csv'.\n",
    "        head_epochs (int): El número de épocas de la Fase 1 (para dibujar la línea).\n",
    "        save_dir (str): Directorio donde se guardarán los gráficos.\n",
    "    \"\"\"\n",
    "    print(f\"Generando gráficos desde {csv_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Leer los datos del CSV\n",
    "        history_df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontró el archivo de historial en {csv_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el CSV: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Gráfico de Precisión (Accuracy) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history_df['epoch'], history_df['train_acc'], label='Precisión (Entrenamiento)')\n",
    "    plt.plot(history_df['epoch'], history_df['val_acc'], label='Precisión (Validación)', linestyle='--')\n",
    "    \n",
    "    # Dibujar línea vertical para separar las fases\n",
    "    # La Fase 1 va de la época 0 a la (head_epochs - 1).\n",
    "    plt.axvline(x=head_epochs - 0.5, color='grey', linestyle=':', label=f'Fin Fase 1 (Época {head_epochs-1})')\n",
    "    \n",
    "    plt.title('Historial de Precisión (Accuracy) del Entrenamiento')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    plt.ylim(0, 1.05) # Asegurar que el eje Y va de 0 a 1 (o 1.05 para margen)\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    acc_plot_path = os.path.join(save_dir, './train_results/training_accuracy_plot.png')\n",
    "    plt.savefig(acc_plot_path)\n",
    "    print(f\"Gráfico de Precisión guardado en: {acc_plot_path}\")\n",
    "    plt.close() # Cerrar la figura para liberar memoria\n",
    "\n",
    "    # --- 3. Gráfico de Pérdida (Loss) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history_df['epoch'], history_df['train_loss'], label='Pérdida (Entrenamiento)')\n",
    "    plt.plot(history_df['epoch'], history_df['val_loss'], label='Pérdida (Validación)', linestyle='--')\n",
    "    \n",
    "    # Dibujar línea vertical\n",
    "    plt.axvline(x=head_epochs - 0.5, color='grey', linestyle=':', label=f'Fin Fase 1 (Época {head_epochs-1})')\n",
    "    \n",
    "    plt.title('Historial de Pérdida (Loss) del Entrenamiento')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    loss_plot_path = os.path.join(save_dir, './train_results/training_loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    print(f\"Gráfico de Pérdida guardado en: {loss_plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# --- Punto de entrada principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Configuración ---\n",
    "    # Asegúrate de que estos valores coincidan con tu script de entrenamiento\n",
    "    CSV_FILE_TO_LOAD = './train_results/training_history_summary.csv'\n",
    "    HEAD_EPOCHS = 15 \n",
    "    # --- Fin de Configuración ---\n",
    "    \n",
    "    plot_training_history(CSV_FILE_TO_LOAD, HEAD_EPOCHS)\n",
    "    print(\"\\nProceso de generación de gráficos finalizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea6642",
   "metadata": {},
   "source": [
    "Tras lo anterior se procede a poner un filtro dependiendo de la edad de la persona detectada a través de la WebCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e78b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ivanp\\AppData\\Local\\Temp\\ipykernel_17604\\514178216.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_age.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ./train_results/best_age_model.pth cargado exitosamente en cpu.\n",
      "Cargando imágenes de filtros...\n",
      "  Filtro './filters/bebe.png' cargado para la edad JOVEN.\n",
      "  Filtro './filters/medio.png' cargado para la edad MEDIO.\n",
      "  Filtro './filters/anciano.png' cargado para la edad ANCIANO.\n",
      "Iniciando cámara... Presiona 'q' para salir.\n",
      "Aplicación cerrada.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "import mediapipe as mp # !NUEVO! Importamos MediaPipe\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# --- 0. Definición del Modelo (NECESARIO en PyTorch) ---\n",
    "def get_model(num_classes=3):\n",
    "    model = models.resnet50(weights=None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# --- 1. Configuración Inicial ---\n",
    "IMG_SIZE = 224\n",
    "MODEL_PATH = './train_results/best_age_model.pth'\n",
    "LABELS = {0: \"JOVEN\", 1: \"MEDIO\", 2: \"ANCIANO\"}\n",
    "BOX_COLOR = (0, 255, 0) \n",
    "\n",
    "# Rutas de filtros\n",
    "FILTER_PATHS = {\n",
    "    0: './filters/bebe.png',\n",
    "    1: './filters/medio.png',  # Tu archivo de \"bigote\"\n",
    "    2: './filters/anciano.png'\n",
    "}\n",
    "\n",
    "# --- 1.5. Configuración de PyTorch ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- 2. Funciones Auxiliares (sin cambios) ---\n",
    "def load_filter_image(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"No se pudo cargar el filtro: {path}\")\n",
    "    if img.shape[2] == 3:\n",
    "        b_channel, g_channel, r_channel = cv2.split(img)\n",
    "        alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 255\n",
    "        img = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n",
    "    return img\n",
    "\n",
    "def overlay_image_alpha(img_base, img_overlay, x, y):\n",
    "    x, y = int(x), int(y)\n",
    "    h, w = img_overlay.shape[0], img_overlay.shape[1]\n",
    "    y1, y2 = max(0, y), min(img_base.shape[0], y + h)\n",
    "    x1, x2 = max(0, x), min(img_base.shape[1], x + w)\n",
    "    y1_overlay, y2_overlay = max(0, -y), min(h, img_base.shape[0] - y)\n",
    "    x1_overlay, x2_overlay = max(0, -x), min(w, img_base.shape[1] - x)\n",
    "    if y1 >= y2 or x1 >= x2 or y1_overlay >= y2_overlay or x1_overlay >= x2_overlay:\n",
    "        return img_base\n",
    "    overlay_cropped = img_overlay[y1_overlay:y2_overlay, x1_overlay:x2_overlay]\n",
    "    img_rgb = overlay_cropped[..., :3]\n",
    "    alpha = overlay_cropped[..., 3:] / 255.0\n",
    "    alpha_inv = 1.0 - alpha\n",
    "    img_base_cropped = img_base[y1:y2, x1:x2]\n",
    "    if img_base_cropped.shape != img_rgb.shape:\n",
    "        return img_base\n",
    "    img_base[y1:y2, x1:x2] = (img_base_cropped * alpha_inv + img_rgb * alpha).astype(np.uint8)\n",
    "    return img_base\n",
    "\n",
    "# --- 3. Cargar Modelos y Filtros ---\n",
    "try:\n",
    "    model_age = get_model(num_classes=len(LABELS))\n",
    "    model_age.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    model_age = model_age.to(device)\n",
    "    model_age.eval()\n",
    "    print(f\"Modelo {MODEL_PATH} cargado exitosamente en {device}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando el modelo PyTorch: {e}\")\n",
    "    exit()\n",
    "\n",
    "loaded_filters = {}\n",
    "print(\"Cargando imágenes de filtros...\")\n",
    "for age_id, path in FILTER_PATHS.items():\n",
    "    try:\n",
    "        loaded_filters[age_id] = load_filter_image(path)\n",
    "        print(f\"  Filtro '{path}' cargado para la edad {LABELS[age_id]}.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"¡ADVERTENCIA! {e}. Este filtro no estará disponible.\")\n",
    "        loaded_filters[age_id] = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error desconocido cargando filtro '{path}': {e}. No disponible.\")\n",
    "        loaded_filters[age_id] = None\n",
    "\n",
    "# --- !NUEVO! Inicializar MediaPipe ---\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "print(\"Iniciando cámara... Presiona 'q' para salir.\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: No se pudo abrir la cámara web.\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. Bucle de Captura de Video ---\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Obtener alto y ancho del frame\n",
    "    frame_h, frame_w, _ = frame.shape\n",
    "\n",
    "    # Convertir a RGB para MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    try:\n",
    "        # --- !MODIFICADO! Detección con MediaPipe ---\n",
    "        results = face_detection.process(rgb_frame)\n",
    "\n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                \n",
    "                # --- !MODIFICADO! Extracción de Bounding Box ---\n",
    "                # MediaPipe da coordenadas normalizadas [0, 1]\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                x = int(bboxC.xmin * frame_w)\n",
    "                y = int(bboxC.ymin * frame_h)\n",
    "                w = int(bboxC.width * frame_w)\n",
    "                h = int(bboxC.height * frame_h)\n",
    "                x, y, w, h = max(0, x), max(0, y), max(0, w), max(0, h) # Asegurar que no sean negativos\n",
    "\n",
    "                # --- !MODIFICADO! Extracción de Landmarks ---\n",
    "                # Extraer los 6 puntos clave de MediaPipe\n",
    "                keypoints = detection.location_data.relative_keypoints\n",
    "                eye_left = (int(keypoints[0].x * frame_w), int(keypoints[0].y * frame_h))\n",
    "                eye_right = (int(keypoints[1].x * frame_w), int(keypoints[1].y * frame_h))\n",
    "                nose = (int(keypoints[2].x * frame_w), int(keypoints[2].y * frame_h))\n",
    "                mouth_center = (int(keypoints[3].x * frame_w), int(keypoints[3].y * frame_h))\n",
    "\n",
    "\n",
    "                try:\n",
    "                    # --- 6. Procesar cada Cara (Sin cambios) ---\n",
    "                    face_roi_rgb = rgb_frame[y:y+h, x:x+w]\n",
    "                    \n",
    "                    if face_roi_rgb.size == 0 or w <= 0 or h <= 0:\n",
    "                        continue\n",
    "\n",
    "                    pil_image = Image.fromarray(face_roi_rgb)\n",
    "                    input_tensor = preprocess_transform(pil_image)\n",
    "                    input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model_age(input_batch)\n",
    "                        probabilities = torch.softmax(outputs, dim=1)\n",
    "                        confidence, class_id_tensor = torch.max(probabilities, 1)\n",
    "\n",
    "                    class_id = class_id_tensor.item()\n",
    "                    confidence_val = confidence.item() * 100\n",
    "                    label_text = LABELS.get(class_id, \"Desconocido\")\n",
    "                    text = f\"{label_text} ({confidence_val:.1f}%)\"\n",
    "\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), BOX_COLOR, 2)\n",
    "                    cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, BOX_COLOR, 2)\n",
    "\n",
    "                    # --- 8. APLICAR FILTRO VISUAL (Usando landmarks de MediaPipe) ---\n",
    "                    selected_filter = loaded_filters.get(class_id)\n",
    "                    if selected_filter is not None:\n",
    "                        \n",
    "                        if class_id == 1: # --- LÓGICA DEL BIGOTE (CLASE 1: \"MEDIO\") ---\n",
    "                            try:\n",
    "                                w_mustache = int(w * 0.6)\n",
    "                                h_filter_orig, w_filter_orig = selected_filter.shape[:2]\n",
    "                                if w_filter_orig == 0: continue\n",
    "                                h_mustache = max(1, int(w_mustache * (h_filter_orig / w_filter_orig)))\n",
    "                                \n",
    "                                if w_mustache > 0 and h_mustache > 0:\n",
    "                                    filter_resized = cv2.resize(selected_filter, (w_mustache, h_mustache))\n",
    "                                    \n",
    "                                    # Posicionar entre la nariz y la boca\n",
    "                                    nose_y = nose[1] # Y del landmark de la nariz\n",
    "                                    mouth_center_y = mouth_center[1] # Y del landmark de la boca\n",
    "                                    center_y_between = int((nose_y + mouth_center_y) / 2)\n",
    "                                    center_x_nose = nose[0] # X del landmark de la nariz\n",
    "                                    \n",
    "                                    x_pos = int(center_x_nose - (w_mustache / 2))\n",
    "                                    y_pos = int(center_y_between - (h_mustache / 2))\n",
    "\n",
    "                                    frame = overlay_image_alpha(frame, filter_resized, x_pos, y_pos)\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                                print(f\"Error al aplicar filtro de bigote: {e}\")\n",
    "\n",
    "                        else: # --- LÓGICA DE BEBÉ (0) Y ANCIANO (2) -> EN LA FRENTE ---\n",
    "                            try:\n",
    "                                center_eyes_x = int((eye_left[0] + eye_right[0]) / 2)\n",
    "                                center_eyes_y = int((eye_left[1] + eye_right[1]) / 2)\n",
    "\n",
    "                                filter_width = max(1, int(w * 0.7)) \n",
    "                                h_filter_orig, w_filter_orig = selected_filter.shape[:2]\n",
    "                                if w_filter_orig == 0: continue\n",
    "                                filter_height = max(1, int(filter_width * (h_filter_orig / w_filter_orig)))\n",
    "                                \n",
    "                                if filter_width > 0 and filter_height > 0:\n",
    "                                    filter_resized = cv2.resize(selected_filter, (filter_width, filter_height))\n",
    "                                    \n",
    "                                    forehead_center_y = int(center_eyes_y - (h * 0.30))\n",
    "                                    x_pos = int(center_eyes_x - (filter_width / 2))\n",
    "                                    y_pos = int(forehead_center_y - (filter_height / 2))\n",
    "\n",
    "                                    frame = overlay_image_alpha(frame, filter_resized, x_pos, y_pos)\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                                print(f\"Error al aplicar filtro de frente: {e}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error procesando cara: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error general en el bucle principal: {e}\")\n",
    "\n",
    "    # --- 8. Mostrar el Resultado ---\n",
    "    cv2.imshow('Detector de Edad con MediaPipe (PyTorch)', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# --- 9. Limpieza ---\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "face_detection.close() # !NUEVO! Cerrar el modelo de MediaPipe\n",
    "print(\"Aplicación cerrada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c32f9b",
   "metadata": {},
   "source": [
    "# FILTRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2aba9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Immagine stellina caricata.\n",
      "[INFO] Avvio del ciclo video con MediaPipe...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "# --- Configuración de Constantes y Estado ---\n",
    "\n",
    "# EAR: Umbral bajo el cual el ojo se considera cerrado\n",
    "EYE_AR_THRESH = 0.25 \n",
    "# Número de frames consecutivos\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "\n",
    "STAR_IMAGE_PATH = \"./filters/star.png\" \n",
    "STAR_SIZE_START = 20 \n",
    "STAR_SIZE_END = 80 \n",
    "STAR_LIFETIME = 1.0 \n",
    "STAR_SPEED = 20 \n",
    "\n",
    "# Estado\n",
    "active_stars = [] \n",
    "COUNTER = 0 \n",
    "BLINKS = 0 \n",
    "\n",
    "# --- Índices de MediaPipe para los ojos ---\n",
    "# MediaPipe usa una malla de 468 puntos. Estos son los índices correspondientes a los ojos.\n",
    "# Orden: [p1, p2, p3, p4, p5, p6] donde p1 y p4 son las esquinas horizontales\n",
    "LEFT_EYE_IDX = [362, 385, 387, 263, 373, 380]\n",
    "RIGHT_EYE_IDX = [33, 160, 158, 133, 153, 144]\n",
    "\n",
    "# --- Funciones Auxiliares ---\n",
    "\n",
    "def eye_aspect_ratio(eye_points):\n",
    "    # Calcula la distancia euclidiana vertical\n",
    "    A = dist.euclidean(eye_points[1], eye_points[5])\n",
    "    B = dist.euclidean(eye_points[2], eye_points[4])\n",
    "    # Calcula la distancia euclidiana horizontal\n",
    "    C = dist.euclidean(eye_points[0], eye_points[3])\n",
    "\n",
    "    # Calcula el EAR\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# --- Inicialización ---\n",
    "\n",
    "# 1. Cargar MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True, # Importante para mayor precisión en ojos\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# 2. Cargar Imagen Estrella\n",
    "try:\n",
    "    star_img_orig = cv2.imread(STAR_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
    "    if star_img_orig is None:\n",
    "        print(f\"[AVISO] No se encontró '{STAR_IMAGE_PATH}'. El efecto no se mostrará.\")\n",
    "    else:\n",
    "        print(f\"[INFO] Immagine stellina caricata.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERRORE] {e}\")\n",
    "    star_img_orig = None\n",
    "\n",
    "# 3. Iniciar Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Errore: Impossibile aprire la webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"[INFO] Avvio del ciclo video con MediaPipe...\")\n",
    "\n",
    "# --- Ciclo Principal ---\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Espejo y dimensiones\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    \n",
    "    # MediaPipe necesita RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Procesar detección\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Convertir landmarks normalizados a píxeles\n",
    "            mesh_points = np.array([np.multiply([p.x, p.y], [w, h]).astype(int) for p in face_landmarks.landmark])\n",
    "\n",
    "            # Obtener coordenadas de los ojos\n",
    "            leftEye = mesh_points[LEFT_EYE_IDX]\n",
    "            rightEye = mesh_points[RIGHT_EYE_IDX]\n",
    "\n",
    "            # Calcular EAR\n",
    "            leftEAR = eye_aspect_ratio(leftEye)\n",
    "            rightEAR = eye_aspect_ratio(rightEye)\n",
    "            ear = (leftEAR + rightEAR) / 2.0\n",
    "\n",
    "            # Dibujar contornos de ojos (Opcional)\n",
    "            cv2.polylines(frame, [leftEye], True, (0, 255, 0), 1)\n",
    "            cv2.polylines(frame, [rightEye], True, (0, 255, 0), 1)\n",
    "\n",
    "            # --- Lógica de Parpadeo ---\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "            else:\n",
    "                if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                    BLINKS += 1\n",
    "                    \n",
    "                    # Generar estrellas si la imagen existe\n",
    "                    if star_img_orig is not None:\n",
    "                        le_center = np.mean(leftEye, axis=0).astype(int)\n",
    "                        re_center = np.mean(rightEye, axis=0).astype(int)\n",
    "\n",
    "                        active_stars.append({\n",
    "                            'start_time': time.time(),\n",
    "                            'x': le_center[0], 'y': le_center[1],\n",
    "                            'initial_pos': le_center\n",
    "                        })\n",
    "                        active_stars.append({\n",
    "                            'start_time': time.time(),\n",
    "                            'x': re_center[0], 'y': re_center[1],\n",
    "                            'initial_pos': re_center\n",
    "                        })\n",
    "                \n",
    "                COUNTER = 0\n",
    "\n",
    "    # --- Renderizado de Animación (Independiente de la detección facial) ---\n",
    "    current_time = time.time()\n",
    "    stars_to_keep = []\n",
    "\n",
    "    for star in active_stars:\n",
    "        elapsed_time = current_time - star['start_time']\n",
    "\n",
    "        if elapsed_time < STAR_LIFETIME:\n",
    "            scale_factor = elapsed_time / STAR_LIFETIME\n",
    "            current_size = int(STAR_SIZE_START + (STAR_SIZE_END - STAR_SIZE_START) * scale_factor)\n",
    "            alpha = 1.0 - scale_factor\n",
    "\n",
    "            # Movimiento\n",
    "            dir_x = (star['x'] - star['initial_pos'][0])\n",
    "            dir_y = (star['y'] - star['initial_pos'][1])\n",
    "            \n",
    "            # Si es el primer frame, dar una dirección aleatoria hacia arriba/afuera o fija\n",
    "            # Aquí mantengo tu lógica original, que asume movimiento radial, \n",
    "            # pero al nacer en el centro, dir_x es 0. Forzamos un movimiento hacia arriba/lados\n",
    "            if dir_x == 0 and dir_y == 0:\n",
    "                 # Truco: Pequeño offset aleatorio para que no se queden quietas\n",
    "                 dir_x = np.random.choice([-1, 1]) \n",
    "                 dir_y = -1 # Hacia arriba\n",
    "\n",
    "            magnitude = math.sqrt(dir_x**2 + dir_y**2)\n",
    "            norm_dir_x, norm_dir_y = (dir_x / magnitude, dir_y / magnitude) if magnitude > 0 else (0, -1)\n",
    "            \n",
    "            move_distance = STAR_SPEED * elapsed_time\n",
    "            # Actualizamos posición base + movimiento\n",
    "            # Nota: Tu lógica original movía la estrella basándose en la posición previa implicita\n",
    "            # Aquí simplifico para que se mueva radialmente desde el ojo\n",
    "            new_x = int(star['initial_pos'][0] + norm_dir_x * move_distance * 20) # *50 factor arbitrario de velocidad visual\n",
    "            new_y = int(star['initial_pos'][1] + norm_dir_y * move_distance * 20)\n",
    "\n",
    "            # Renderizado (Blending)\n",
    "            if star_img_orig is not None and current_size > 0:\n",
    "                resized_star = cv2.resize(star_img_orig, (current_size, current_size))\n",
    "                \n",
    "                x1 = new_x - current_size // 2\n",
    "                y1 = new_y - current_size // 2\n",
    "                x2 = x1 + current_size\n",
    "                y2 = y1 + current_size\n",
    "\n",
    "                y1_frame = max(0, y1)\n",
    "                x1_frame = max(0, x1)\n",
    "                y2_frame = min(frame.shape[0], y2)\n",
    "                x2_frame = min(frame.shape[1], x2)\n",
    "\n",
    "                y1_star = y1_frame - y1\n",
    "                x1_star = x1_frame - x1\n",
    "                y2_star = y1_star + (y2_frame - y1_frame)\n",
    "                x2_star = x1_star + (x2_frame - x1_frame)\n",
    "\n",
    "                if x2_frame > x1_frame and y2_frame > y1_frame:\n",
    "                    roi_frame = frame[y1_frame:y2_frame, x1_frame:x2_frame].astype(np.float32)\n",
    "                    roi_star = resized_star[y1_star:y2_star, x1_star:x2_star].astype(np.float32)\n",
    "                    \n",
    "                    blended = roi_frame * (1.0 - alpha) + roi_star * alpha\n",
    "                    frame[y1_frame:y2_frame, x1_frame:x2_frame] = blended.astype(np.uint8)\n",
    "\n",
    "            stars_to_keep.append(star)\n",
    "    \n",
    "    active_stars = stars_to_keep\n",
    "\n",
    "    # UI Info\n",
    "    cv2.putText(frame, f\"Blinks: {BLINKS}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"MediaPipe Blink Filter\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
