{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55e596e",
   "metadata": {},
   "source": [
    "# **Cuarto conjunto de tareas a realizar**\n",
    "\n",
    "## Paquetes necesarios e inicializaciones\n",
    "\n",
    "La siguiente práctica consta dos partes principales, la primera de ellas basada en YOLO y en detección de matrículas personas y vehículos y la segunda en OCR (Optical Character Recognition)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "766bb752",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "conda create --name VC_P4 python=3.9.5\n",
    "conda activate VC_P4\n",
    "pip install ultralytics\n",
    "pip install lapx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049c46d",
   "metadata": {},
   "source": [
    "Si se tiene una tarjeta gráfica de NVIDIA se puede utilizar la GPU haciendo uso de CUDA, para instalar CUDAv11.6 hacer uso del siguiente script."
   ]
  },
  {
   "cell_type": "raw",
   "id": "677c975c",
   "metadata": {},
   "source": [
    "conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.6 -c pytorch -c conda-forge"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9cbcd16",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "yolo detect predict model=yolo11n.pt source=\"rutaimg_video\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34f05b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import yaml\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55496679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento del modelo\n",
      "Ultralytics 8.3.223  Python-3.9.23 torch-2.8.0+cpu CPU (12th Gen Intel Core i7-12700H)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=exp12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train_custom, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\ivanp\\Desktop\\Visin por Computador\\Computer_Vision\\VC_P4\\runs\\train_custom\\exp12, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=15\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    433597  ultralytics.nn.modules.head.Detect           [15, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,592,765 parameters, 2,592,749 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 32.49.4 MB/s, size: 23.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ivanp\\Desktop\\traffic_signals\\TGC_RBNW\\train\\labels.cache... 3530 images, 3 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 3530/3530  0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.2 ms, read: 15.18.4 MB/s, size: 21.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ivanp\\Desktop\\traffic_signals\\TGC_RBNW\\val\\labels.cache... 801 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 801/801  0.0s\n",
      "Plotting labels to C:\\Users\\ivanp\\Desktop\\Visin por Computador\\Computer_Vision\\VC_P4\\runs\\train_custom\\exp12\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\ivanp\\Desktop\\Visin por Computador\\Computer_Vision\\VC_P4\\runs\\train_custom\\exp12\u001b[0m\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/40         0G       1.26      4.261      1.284         10        416: 1% ──────────── 8/883 0.6it/s 14.0s<22:304\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Entrena con tu dataset y configuración\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns/train_custom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexp1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenamiento completado.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\ultralytics\\engine\\model.py:800\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m--> 800\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\ultralytics\\engine\\trainer.py:240\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\ultralytics\\engine\\trainer.py:431\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_step()\n",
      "File \u001b[1;32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ivanp\\anaconda3\\envs\\VC_P4\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === ENTRENAMIENTO DEL MODELO ===\n",
    "print(\"Iniciando entrenamiento del modelo\")\n",
    "\n",
    "# Carga el modelo base (preentrenado)\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Entrena con tu dataset y configuración\n",
    "results = model.train(\n",
    "    data=\"data.yaml\",\n",
    "    imgsz=416,\n",
    "    batch=4,\n",
    "    device=\"cpu\",\n",
    "    epochs=40,\n",
    "    project=\"runs/train_custom\",\n",
    "    name=\"exp1\",\n",
    "    exist_ok=False\n",
    ")\n",
    "\n",
    "print(\"Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cc4a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CÁLCULO MANUAL DESDE MATRIZ DE CONFUSIÓN:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCÁLCULO MANUAL DESDE MATRIZ DE CONFUSIÓN:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Obtén la matriz de confusión (es un array de NumPy)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m matrix \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mconfusion_matrix\u001b[38;5;241m.\u001b[39mmatrix\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m matrix\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 2. Calcula la suma de la diagonal (aciertos correctos)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# np.diag() extrae la diagonal\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     aciertos_correctos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(matrix)\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"CÁLCULO MANUAL DESDE MATRIZ DE CONFUSIÓN:\")\n",
    "\n",
    "# 1. Obtén la matriz de confusión (es un array de NumPy)\n",
    "matrix = results.confusion_matrix.matrix\n",
    "\n",
    "if matrix is not None and matrix.size > 0:\n",
    "    # 2. Calcula la suma de la diagonal (aciertos correctos)\n",
    "    # np.diag() extrae la diagonal\n",
    "    aciertos_correctos = np.diag(matrix).sum()\n",
    "\n",
    "    # 3. Calcula el total de predicciones (suma de toda la matriz)\n",
    "    total_predicciones = matrix.sum()\n",
    "\n",
    "    # 4. Calcula el acierto\n",
    "    if total_predicciones > 0:\n",
    "        accuracy_manual = aciertos_correctos / total_predicciones\n",
    "        \n",
    "        print(f\"Matriz de Confusión: \\n{matrix}\")\n",
    "        print(f\"Aciertos (Diagonal): {aciertos_correctos}\")\n",
    "        print(f\"Total de Muestras: {total_predicciones}\")\n",
    "        print(f\"Porcentaje de Acierto (Accuracy Manual): {accuracy_manual * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"La matriz de confusión está vacía.\")\n",
    "else:\n",
    "    print(\"No se generó matriz de confusión (quizás no es un modelo de detección/clasificación).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9a3d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases personalizadas: ['Green Light', 'Red Light', 'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110', 'Speed Limit 120', 'Speed Limit 20', 'Speed Limit 30', 'Speed Limit 40', 'Speed Limit 50', 'Speed Limit 60', 'Speed Limit 70', 'Speed Limit 80', 'Speed Limit 90', 'Stop']\n",
      "Iniciando detección y tracking\n",
      "Guardando resultados en tracking_results.csv...\n",
      "\n",
      "0: 416x416 1 Stop, 136.8ms\n",
      "Speed: 13.1ms preprocess, 136.8ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 161.5ms\n",
      "Speed: 1.8ms preprocess, 161.5ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 131.4ms\n",
      "Speed: 2.8ms preprocess, 131.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 105.9ms\n",
      "Speed: 1.4ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 130.6ms\n",
      "Speed: 2.1ms preprocess, 130.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 121.0ms\n",
      "Speed: 1.3ms preprocess, 121.0ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 100.8ms\n",
      "Speed: 2.4ms preprocess, 100.8ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 90.9ms\n",
      "Speed: 1.6ms preprocess, 90.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 110.8ms\n",
      "Speed: 1.5ms preprocess, 110.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 10, 1 Speed Limit 20, 2 Speed Limit 50s, 91.2ms\n",
      "Speed: 2.0ms preprocess, 91.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 110.3ms\n",
      "Speed: 2.3ms preprocess, 110.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 10, 112.0ms\n",
      "Speed: 1.5ms preprocess, 112.0ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 102.5ms\n",
      "Speed: 2.3ms preprocess, 102.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 92.0ms\n",
      "Speed: 1.8ms preprocess, 92.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 127.5ms\n",
      "Speed: 1.9ms preprocess, 127.5ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 172.4ms\n",
      "Speed: 1.9ms preprocess, 172.4ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 150.4ms\n",
      "Speed: 1.9ms preprocess, 150.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 101.6ms\n",
      "Speed: 1.4ms preprocess, 101.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 118.1ms\n",
      "Speed: 1.9ms preprocess, 118.1ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 20, 1 Speed Limit 30, 1 Speed Limit 50, 1 Speed Limit 80, 111.4ms\n",
      "Speed: 1.5ms preprocess, 111.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 148.9ms\n",
      "Speed: 1.9ms preprocess, 148.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Stop, 117.9ms\n",
      "Speed: 3.1ms preprocess, 117.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 120, 1 Speed Limit 20, 90.2ms\n",
      "Speed: 1.4ms preprocess, 90.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 1 Stop, 120.9ms\n",
      "Speed: 1.7ms preprocess, 120.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 118.7ms\n",
      "Speed: 1.9ms preprocess, 118.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 3 Speed Limit 20s, 1 Speed Limit 50, 110.5ms\n",
      "Speed: 1.6ms preprocess, 110.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 167.3ms\n",
      "Speed: 1.9ms preprocess, 167.3ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 102.6ms\n",
      "Speed: 2.7ms preprocess, 102.6ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 2 Speed Limit 50s, 1 Speed Limit 80, 118.1ms\n",
      "Speed: 1.5ms preprocess, 118.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 87.5ms\n",
      "Speed: 2.0ms preprocess, 87.5ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 90.0ms\n",
      "Speed: 2.1ms preprocess, 90.0ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 2 Speed Limit 50s, 85.5ms\n",
      "Speed: 1.1ms preprocess, 85.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 89.0ms\n",
      "Speed: 1.7ms preprocess, 89.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 50, 85.7ms\n",
      "Speed: 1.0ms preprocess, 85.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 131.9ms\n",
      "Speed: 1.6ms preprocess, 131.9ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Speed Limit 20s, 1 Speed Limit 30, 2 Speed Limit 50s, 131.1ms\n",
      "Speed: 2.4ms preprocess, 131.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 30, 2 Speed Limit 50s, 128.5ms\n",
      "Speed: 1.8ms preprocess, 128.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 101.6ms\n",
      "Speed: 1.1ms preprocess, 101.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 10, 1 Speed Limit 20, 1 Speed Limit 30, 3 Speed Limit 50s, 103.6ms\n",
      "Speed: 2.8ms preprocess, 103.6ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 30, 2 Speed Limit 50s, 102.6ms\n",
      "Speed: 1.5ms preprocess, 102.6ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 50, 102.8ms\n",
      "Speed: 1.4ms preprocess, 102.8ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 101.1ms\n",
      "Speed: 1.7ms preprocess, 101.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 100.4ms\n",
      "Speed: 1.4ms preprocess, 100.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 20, 1 Speed Limit 30, 2 Speed Limit 50s, 85.7ms\n",
      "Speed: 1.2ms preprocess, 85.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 50, 100.7ms\n",
      "Speed: 1.6ms preprocess, 100.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 30, 1 Speed Limit 50, 105.1ms\n",
      "Speed: 1.8ms preprocess, 105.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 132.3ms\n",
      "Speed: 1.5ms preprocess, 132.3ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Red Light, 1 Speed Limit 20, 128.2ms\n",
      "Speed: 1.4ms preprocess, 128.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 118.6ms\n",
      "Speed: 1.2ms preprocess, 118.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 30, 2 Speed Limit 50s, 131.5ms\n",
      "Speed: 1.6ms preprocess, 131.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 129.4ms\n",
      "Speed: 1.8ms preprocess, 129.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Speed Limit 110s, 1 Speed Limit 30, 1 Speed Limit 50, 1 Speed Limit 70, 127.1ms\n",
      "Speed: 1.4ms preprocess, 127.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Speed Limit 110s, 2 Speed Limit 30s, 2 Speed Limit 50s, 1 Speed Limit 70, 106.3ms\n",
      "Speed: 1.7ms preprocess, 106.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Speed Limit 50s, 137.9ms\n",
      "Speed: 2.9ms preprocess, 137.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 50, 136.0ms\n",
      "Speed: 1.4ms preprocess, 136.0ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 40, 1 Speed Limit 50, 137.9ms\n",
      "Speed: 2.8ms preprocess, 137.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 30, 1 Speed Limit 50, 141.7ms\n",
      "Speed: 1.5ms preprocess, 141.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 30, 1 Speed Limit 50, 114.0ms\n",
      "Speed: 1.5ms preprocess, 114.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 30, 1 Speed Limit 50, 117.3ms\n",
      "Speed: 1.8ms preprocess, 117.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 30, 1 Speed Limit 40, 211.1ms\n",
      "Speed: 2.9ms preprocess, 211.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 147.6ms\n",
      "Speed: 14.9ms preprocess, 147.6ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 30, 2 Speed Limit 50s, 1 Speed Limit 80, 220.6ms\n",
      "Speed: 2.5ms preprocess, 220.6ms inference, 2.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 30, 2 Speed Limit 50s, 182.1ms\n",
      "Speed: 2.4ms preprocess, 182.1ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 141.3ms\n",
      "Speed: 2.0ms preprocess, 141.3ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 120, 1 Speed Limit 50, 106.5ms\n",
      "Speed: 1.5ms preprocess, 106.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 96.9ms\n",
      "Speed: 1.5ms preprocess, 96.9ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 96.9ms\n",
      "Speed: 1.4ms preprocess, 96.9ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 99.9ms\n",
      "Speed: 1.4ms preprocess, 99.9ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Speed Limit 50s, 98.7ms\n",
      "Speed: 1.4ms preprocess, 98.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 20, 1 Speed Limit 50, 99.7ms\n",
      "Speed: 2.1ms preprocess, 99.7ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 103.4ms\n",
      "Speed: 2.0ms preprocess, 103.4ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 102.7ms\n",
      "Speed: 1.5ms preprocess, 102.7ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 30, 1 Speed Limit 40, 1 Speed Limit 50, 101.2ms\n",
      "Speed: 1.4ms preprocess, 101.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 95.8ms\n",
      "Speed: 1.8ms preprocess, 95.8ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 30, 1 Speed Limit 40, 168.9ms\n",
      "Speed: 1.9ms preprocess, 168.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 30, 122.1ms\n",
      "Speed: 2.9ms preprocess, 122.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 30, 1 Speed Limit 40, 1 Speed Limit 70, 92.4ms\n",
      "Speed: 1.3ms preprocess, 92.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 87.3ms\n",
      "Speed: 1.3ms preprocess, 87.3ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 89.8ms\n",
      "Speed: 1.5ms preprocess, 89.8ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 85.1ms\n",
      "Speed: 2.8ms preprocess, 85.1ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 87.2ms\n",
      "Speed: 1.4ms preprocess, 87.2ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 86.7ms\n",
      "Speed: 1.0ms preprocess, 86.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 89.1ms\n",
      "Speed: 1.4ms preprocess, 89.1ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 80, 90.4ms\n",
      "Speed: 1.5ms preprocess, 90.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 85.9ms\n",
      "Speed: 1.3ms preprocess, 85.9ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 80, 86.9ms\n",
      "Speed: 1.3ms preprocess, 86.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 80, 89.6ms\n",
      "Speed: 1.6ms preprocess, 89.6ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 80, 1 Speed Limit 90, 86.7ms\n",
      "Speed: 1.3ms preprocess, 86.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 1 Speed Limit 80, 1 Speed Limit 90, 106.8ms\n",
      "Speed: 1.8ms preprocess, 106.8ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 117.2ms\n",
      "Speed: 1.7ms preprocess, 117.2ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 95.2ms\n",
      "Speed: 1.5ms preprocess, 95.2ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Green Lights, 101.1ms\n",
      "Speed: 2.7ms preprocess, 101.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 92.9ms\n",
      "Speed: 1.5ms preprocess, 92.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 147.5ms\n",
      "Speed: 1.5ms preprocess, 147.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Green Lights, 1 Speed Limit 50, 122.1ms\n",
      "Speed: 1.4ms preprocess, 122.1ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Green Lights, 1 Speed Limit 50, 104.0ms\n",
      "Speed: 1.1ms preprocess, 104.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 (no detections), 93.5ms\n",
      "Speed: 1.5ms preprocess, 93.5ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Green Lights, 154.7ms\n",
      "Speed: 2.3ms preprocess, 154.7ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Green Lights, 97.0ms\n",
      "Speed: 1.2ms preprocess, 97.0ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 20, 145.2ms\n",
      "Speed: 2.2ms preprocess, 145.2ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 1 Red Light, 199.1ms\n",
      "Speed: 2.4ms preprocess, 199.1ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 2 Speed Limit 110s, 178.1ms\n",
      "Speed: 2.7ms preprocess, 178.1ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 140.9ms\n",
      "Speed: 3.3ms preprocess, 140.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 109.0ms\n",
      "Speed: 1.3ms preprocess, 109.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 101.5ms\n",
      "Speed: 1.5ms preprocess, 101.5ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 80, 98.8ms\n",
      "Speed: 1.4ms preprocess, 98.8ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 101.4ms\n",
      "Speed: 2.3ms preprocess, 101.4ms inference, 1.0ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 110, 1 Speed Limit 80, 101.7ms\n",
      "Speed: 1.6ms preprocess, 101.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Speed Limit 50, 1 Speed Limit 80, 179.7ms\n",
      "Speed: 1.6ms preprocess, 179.7ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 416)\n",
      "\n",
      "0: 416x416 1 Green Light, 215.8ms\n",
      "Speed: 2.0ms preprocess, 215.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 416)\n",
      "Detección interrumpida por el usuario.\n",
      "Proceso finalizado. Resultados guardados en tracking_results.csv.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from ultralytics import YOLO\n",
    "import csv\n",
    "\n",
    "# === CARGAR CLASES PERSONALIZADAS ===\n",
    "with open(\"data.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "classNames = data[\"names\"]\n",
    "\n",
    "print(f\"Clases personalizadas: {classNames}\")\n",
    "\n",
    "# === CARGAR EL MODELO ENTRENADO ===\n",
    "model = YOLO(\"runs/train_custom/exp1/weights/best.pt\")\n",
    "\n",
    "# === TRACKING EN VÍDEO ===\n",
    "print(\"Iniciando detección y tracking\")\n",
    "\n",
    "video_path = \"./Resources/test.mp4\"\n",
    "vid = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not vid.isOpened():\n",
    "    print(\"Error: no se pudo abrir el video.\")\n",
    "    exit()\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# --- Definir el nuevo tamaño fijo ---\n",
    "new_width = 1000\n",
    "new_height = 700\n",
    "\n",
    "# <<<<< 1. Preparar el archivo CSV para guardar los resultados\n",
    "csv_filename = \"tracking_results.csv\"\n",
    "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    # <<<<< 2. MODIFICADO: Añadir coordenadas normalizadas al encabezado\n",
    "    csv_writer.writerow(['frame', 'objeto detectado', 'confianza', 'id', 'x_center_norm', 'y_center_norm'])\n",
    "    \n",
    "    print(f\"Guardando resultados en {csv_filename}...\")\n",
    "\n",
    "    frame_nmr = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            print(\"Fin del video.\")\n",
    "            break\n",
    "            \n",
    "        frame_nmr += 1\n",
    "        results = model.track(frame, persist=True, stream=True)\n",
    "\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                # --- Coordenadas en Píxeles (para dibujar y trazar historial) ---\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                x_center = int((x1 + x2) / 2)\n",
    "                y_center = int((y1 + y2) / 2)\n",
    "\n",
    "                # <<<<< 3. NUEVO: Obtener coordenadas normalizadas\n",
    "                # box.xywhn[0] devuelve [x_centro_norm, y_centro_norm, ancho_norm, alto_norm]\n",
    "                norm_coords = box.xywhn[0]\n",
    "                x_center_norm = round(float(norm_coords[0]), 6) # Redondeamos a 6 decimales\n",
    "                y_center_norm = round(float(norm_coords[1]), 6) # Redondeamos a 6 decimales\n",
    "\n",
    "                # --- Resto de los datos ---\n",
    "                confidence = math.ceil((box.conf[0] * 100)) / 100\n",
    "                cls = int(box.cls[0])\n",
    "                class_name = classNames[cls]\n",
    "\n",
    "                track_id = \"\"\n",
    "                if hasattr(box, \"id\") and box.id is not None:\n",
    "                    track_id = str(int(box.id[0].tolist()))\n",
    "\n",
    "                # <<<<< 4. MODIFICADO: Escribir coordenadas normalizadas en el CSV\n",
    "                csv_writer.writerow([frame_nmr, class_name, confidence, track_id, x_center_norm, y_center_norm])\n",
    "\n",
    "                # --- (El resto de tu código de dibujo sigue igual) ---\n",
    "                \n",
    "                escala = int((cls / len(classNames)) * 255 * 3)\n",
    "                if escala >= 255 * 2: R, G, B = 255, 255, escala - 255 * 2\n",
    "                elif escala >= 255: R, G, B = 255, escala - 255, 0\n",
    "                else: R, G, B = escala, 0, 0\n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (R, G, B), 3)\n",
    "                cv2.putText(\n",
    "                    frame, f\"{track_id} {class_name} {confidence}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2\n",
    "                )\n",
    "\n",
    "                if hasattr(box, \"id\") and box.id is not None:\n",
    "                    tid = int(box.id[0])\n",
    "                    # (x_center, y_center) en píxeles se siguen usando aquí\n",
    "                    track = track_history[tid]\n",
    "                    track.append((x_center, y_center))\n",
    "                    if len(track) > 30: track.pop(0)\n",
    "                    points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                    cv2.polylines(frame, [points], isClosed=False, color=(230, 230, 230), thickness=2)\n",
    "\n",
    "        \n",
    "        # <<<<< AQUÍ ES DONDE SE REDIMENSIONA EL VÍDEO >>>>>\n",
    "        \n",
    "        # --- Opción B: Usar un tamaño fijo ---\n",
    "        if frame.shape[1] > 0 and frame.shape[0] > 0:\n",
    "            resized_frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "        else:\n",
    "            resized_frame = frame # Usar el original si hay error\n",
    "\n",
    "        # Muestra el frame redimensionado\n",
    "        cv2.imshow(\"YOLO Tracking - Modelo Personalizado\", resized_frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord(\"q\") or key == 27:\n",
    "            print(\"Detección interrumpida por el usuario.\")\n",
    "            break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Proceso finalizado. Resultados guardados en {csv_filename}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
